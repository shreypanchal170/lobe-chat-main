# 自定义助手指南

#### TOC

- [添加自定义助手](#添加自定义助手)
  - [`A` 通过角色市场添加](#a-通过角色市场添加)
  - [`B` 通过新建自定义助手](#b-通过新建自定义助手)
- [Prompt 基本概念](#prompt-基本概念)
  - [如何写好一个结构化 prompt](#如何写好一个结构化-prompt)
  - [如何提升其质量和效果](#如何提升其质量和效果)
- [模型的概念](#模型的概念)
  - [ChatGPT](#chatgpt)
- [模型参数概念](#模型参数概念)
  - [`temperature`](#temperature)
  - [`top_p`](#top_p)
  - [`presence_penalty`](#presence_penalty)
  - [`frequency_penalty`](#frequency_penalty)
- [扩展阅读](#扩展阅读)

## 添加自定义助手

作为 LobeChat 的基础职能单位，助手的添加和迭代是非常重要的。现在你可以通过两种方式将助手添加到你的常用列表中

### `A` 通过角色市场添加

如果你是一个 Prompt 编写的新手，不妨先浏览一下 LobeChat 的助手市场。在这里，你可以找到其他人提交的常用助手，并且只需一键添加到你的列表中，非常方便。

![](https://github-production-user-asset-6210df.s3.amazonaws.com/17870709/279588466-4c32041b-a8e6-4703-ba4a-f91b7800e359.png)

### `B` 通过新建自定义助手

当你需要处理一些特定的任务时，你就需要考虑创建一个自定义助手来帮助你解决问题。可以通过以下方式添加并进行助手的详细配置

![](https://github-production-user-asset-6210df.s3.amazonaws.com/17870709/279587283-a3ea8dfd-70fb-47ee-ab00-e3911ac6a939.png)
![](https://github-production-user-asset-6210df.s3.amazonaws.com/17870709/279587292-a3d102c6-f61e-4578-91f1-c0a4c97588e1.png)

> \[!NOTE]
>
> 快捷设置技巧：可以通过侧边栏的快捷编辑按钮进行 Prompt 的便捷修改

![](https://github-production-user-asset-6210df.s3.amazonaws.com/17870709/279587294-388d1877-193e-4a50-9fe8-8fbcc3ccefa0.png)
![](https://github-production-user-asset-6210df.s3.amazonaws.com/17870709/279587298-333da153-13b8-4557-a0a2-cff55e7bc1c0.png)

请继续阅读下文，理解 Prompt 编写技巧和常见的模型参数设置

<br/>

## Prompt 基本概念

生成式 AI 非常有用，但它需要人类指导。通常情况下，生成式 AI 能就像公司新来的实习生一样，非常有能力，但需要清晰的指示才能做得好。能够正确地指导生成式 AI 是一项非常强大的技能。你可以通过发送一个 prompt 来指导生成式 AI，这通常是一个文本指令。Prompt 是向助手提供的输入，它会影响输出结果。一个好的 Prompt 应该是结构化的，清晰的，简洁的，并且具有指向性。

### 如何写好一个结构化 prompt

> \[!TIP]
>
> 结构化 prompt 是指 prompt 的构造应该有明确的逻辑和结构。例如，如果你想让模型生成一篇文章，你的 prompt 可能需要包括文章的主题，文章的大纲，文章的风格等信息。

让我们看一个基本的讨论问题的例子:

> _"我们星球面临的最紧迫的环境问题是什么，个人可以采取哪些措施来帮助解决这些问题？"_

我们可以将其转化为简单的助手提示，将回答以下问题：放在前面。

```
回答以下问题:
我们星球面临的最紧迫的环境问题是什么，个人可以采取哪些措施来帮助解决这些问题？
```

由于这个提示生成的结果并不一致，有些只有一两个句子。一个典型的讨论回答应该有多个段落，因此这些结果并不理想。一个好的提示应该给出**具体的格式和内容指令**。您需要消除语言中的歧义以提高一致性和质量。这是一个更好的提示。

```
写一篇高度详细的论文，包括引言、正文和结论段，回答以下问题:
我们星球面临的最紧迫的环境问题是什么，
个人可以采取哪些措施来帮助解决这些问题？
```

第二个提示生成了更长的输出和更好的结构。提示中使用 “论文” 一词是有意的，因为助手可以理解论文的定义，因此更有可能生成连贯的、结构化的回答。

<br/>

### 如何提升其质量和效果

> \[!TIP]
>
> 提升 prompt 质量和效果的方法主要有以下几点：
>
> - 尽量明确你的需求。模型的输出会尽可能满足你的需求，所以如果你的需求不明确，输出可能会不如预期。
> - 使用正确的语法和拼写。模型会尽可能模仿你的语言风格，所以如果你的语言风格有问题，输出可能也会有问题。
> - 提供足够的上下文信息。模型会根据你提供的上下文信息生成输出，所以如果你提供的上下文信息不足，可能无法生成你想要的结果。

在为讨论问题制定有效的提示后，您现在需要细化生成的结果。这可能涉及到调整输出以符合诸如字数等限制，或将不同生成的结果的概念组合在一起。

迭代的一个简单方法是生成多个输出并查看它们，以了解正在使用的概念和结构。一旦评估了输出，您就可以选择最合适的输出并将它们组合成一个连贯的回答。另一种迭代的方法是逐步开始，然后**逐步扩展**。这需要不止一个提示：一个起始提示，用于撰写最初的一两段，然后是其他提示，以扩展已经写过的内容。以下是一个潜在的哲学讨论问题:

> _"数学是发明还是发现？用仔细的推理来解释你的答案。"_

将其添加到一个简单的提示中，如下所示:

```
写一个起始段回答以下问题:
数学是发明还是发现？用仔细的推理来解释你的答案。
```

我生成了一些回答，找到了一个我喜欢的：

```
哲学家和数学家之间争论不休，数学是发明还是发现。
一些人认为，数学的概念和原则独立于人类思维存在，因此可以被认为是被发现而不是被发明的。
例如，数字、比率和几何形状的概念在自然界中被观察到，可以被抽象化和系统化地研究。
```

是一个不错的开端。然后我使用另一个提示来扩展：

```
我正在撰写一篇详细的短文，回答以下提示:
数学是发明还是发现？用仔细的推理来解释你的答案。这是我目前的情况: 哲学家和数学家之间争论不休，数学是发明还是发现。
一些人认为，数学的概念和原则独立于人类思维存在，因此可以被认为是被发现而不是被发明的。
例如，数字、比率和几何形状的概念在自然界中被观察到，可以被抽象化和系统化地研究。写下我文章的下一段。
```

我使用此提示生成了几个扩展，并选择了一个我喜欢的，最终得到了以下结果：

```
哲学家和数学家之间争论不休，数学是发明还是发现。
一些人认为，数学的概念和原则独立于人类思维存在，因此可以被认为是被发现而不是被发明的。
例如，数字、比率和几何形状的概念在自然界中被观察到，可以被抽象化和系统化地研究。
另一方面，有人认为数学在很大程度上是人类思维的发明。
数学原理基于逻辑推理，这是人类思维的一个特征。
例如，欧几里德几何基于抽象的公理和前提，被接受为真实而无需证明。
因此，可以认为几何是人类思维的发明，而不是一种发现。
同样，数学公式和方程用于模拟和预测物理现象，这又是人类推理的结果。
```

使用扩展提示，我们可以逐步地写作并在每个步骤上进行迭代。这对于需要**生成更高质量的输出并希望逐步修改**的情况非常有用。

<br/>

## 模型的概念

### ChatGPT

- **gpt-3.5-turbo**：目前最生成速度最快的 chatgpt 模型更快，但可能会牺牲一些生成文本的质量，上下文长度为 4k。
- **gpt-3.5-turbo-16k**：同 gpt-4，上下文限制增加到 16k token，同时费率更高。
- **gpt-4**：ChatGPT 4.0 在语言理解和生成能力方面相对于 3.5 有所提升。它可以更好地理解上下文和语境，并生成更准确、自然的回答。这得益于 GPT-4 模型的改进，包括更好的语言建模和更深入的语义理解，但它的速度可能比其他模型慢，上下文长度为 8k。
- **gpt-4-32k**：同 gpt-4，上下文限制增加到 32k token，同时费率更高。

<br/>

## 模型参数概念

LLM 看似很神奇，但本质还是一个概率问题，神经网络根据输入的文本，从预训练的模型里面生成一堆候选词，选择概率高的作为输出，相关的参数，大多都是跟采样有关（也就是要如何从候选词里选择输出）。

### `temperature`

用于控制模型输出的结果的随机性，这个值越大随机性越大。一般我们多次输入相同的 prompt 之后，模型的每次输出都不一样。

- 设置为 0，对每个 prompt 都生成固定的输出
- 较低的值，输出更集中，更有确定性
- 较高的值，输出更随机（更有创意 ）

> \[!NOTE]
>
> 一般来说，prompt 越长，描述得越清楚，模型生成的输出质量就越好，置信度越高，这时可以适当调高 temperature 的值；反过来，如果 prompt 很短，很含糊，这时再设置一个比较高的 temperature 值，模型的输出就很不稳定了。

<br/>

### `top_p`

核采样 top_p 也是采样参数，跟 temperature 不一样的采样方式。模型在输出之前，会生成一堆 token，这些 token 根据质量高低排名，核采样模式中候选词列表是动态的，从 tokens 里按百分比选择候选词。 top-p 为选择 token 引入了随机性，让其他高分的 token 有被选择的机会，不会总是选最高分的。

> \[!NOTE]
>
> top_p 与随机性类似，一般来说不建议和随机性 temperature 一起更改

<br/>

### `presence_penalty`

Presence Penalty 参数可以看作是对生成文本中重复内容的一种惩罚。当该参数设置较高时，生成模型会尽量避免产生重复的词语、短语或句子。相反，如果 Presence Penalty 参数较低，则生成的文本可能会包含更多重复的内容。通过调整 Presence Penalty 参数的值，可以实现对生成文本的原创性和多样性的控制。参数的重要性主要体现在以下几个方面：

- 提高生成文本的独创性和多样性：在某些应用场景下，如创意写作、生成新闻标题等，需要生成的文本具有较高的独创性和多样性。通过增加 Presence Penalty 参数的值，可以有效减少生成文本中的重复内容，从而提高文本的独创性和多样性。
- 防止生成循环和无意义的内容：在某些情况下，生成模型可能会产生循环、重复的文本，这些文本通常无法传达有效的信息。通过适当增加 Presence Penalty 参数的值，可以降低生成这类无意义内容的概率，提高生成文本的可读性和实用性。

> \[!NOTE]
>
> 值得注意的是，Presence Penalty 参数与其他参数（如 Temperature 和 top-p）共同影响着生成文本的质量。对比其他参数，Presence Penalty 参数主要关注文本的独创性和重复性，而 Temperature 和 top-p 参数则更多地影响着生成文本的随机性和确定性。通过合理地调整这些参数，可以实现对生成文本质量的综合控制

<br/>

### `frequency_penalty`

是一种机制，通过对文本中频繁出现的新词汇施加惩罚，以减少模型重复同一词语的可能性，值越大，越有可能降低重复字词。

- `-2.0` 当早间新闻开始播出，我发现我家电视现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在现在 _（频率最高的词是 “现在”，占比 44.79%）_
- `-1.0` 他总是在清晨看新闻，在电视前看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看看 _（频率最高的词是 “看”，占比 57.69%）_
- `0.0` 当清晨的阳光洒进小餐馆时，一名疲倦的邮递员出现在门口，他的手中提着一袋信件。店主热情地为他准备了一份早餐，他在享用早餐的同时开始整理邮件。**（频率最高的词是 “的”，占比 8.45%）**
- `1.0` 一个深度睡眠的女孩被一阵温暖的阳光唤醒，她看到了早晨的第一缕阳光，周围是鸟语花香，一切都充满了生机。_（频率最高的词是 “的”，占比 5.45%）_
- `2.0` 每天早上，他都会在阳台上坐着吃早餐。在柔和的夕阳照耀下，一切看起来都非常宁静。然而有一天，当他准备端起早餐的时候，一只乐观的小鸟飞过，给他带来了一天的好心情。 _（频率最高的词是 “的”，占比 4.94%）_

## 扩展阅读

- **Learn Prompting** - <https://learnprompting.org/zh-Hans/docs/intro>
